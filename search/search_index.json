{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PolyFuzz \u00b6 PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models.","title":"Home"},{"location":"#polyfuzz","text":"PolyFuzz performs fuzzy string matching, string grouping, and contains extensive evaluation functions. PolyFuzz is meant to bring fuzzy string matching techniques together within a single framework. Currently, methods include Levenshtein distance with RapidFuzz, a character-based n-gram TF-IDF, word embedding techniques such as FastText and GloVe, and \ud83e\udd17 transformers embeddings. The philosophy of PolyFuzz is: Easy to use yet highly customizable . It is a string matcher tool that requires only a few lines of code but that allows you customize and create your own models.","title":"PolyFuzz"},{"location":"releases/","text":"v0.4.0 \u00b6 Added new models (SentenceTransformers, Gensim, USE, Spacy) Added .fit , .transform , and .fit_transform methods Added .save and PolyFuzz.load() SentenceTransformers from polyfuzz.models import SentenceEmbeddings distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" ) model = PolyFuzz ( distance_model ) Gensim from polyfuzz.models import GensimEmbeddings distance_model = GensimEmbeddings ( \"glove-twitter-25\" ) model = PolyFuzz ( distance_model ) USE from polyfuzz.models import USEEmbeddings distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) model = PolyFuzz ( distance_model ) Spacy from polyfuzz.models import SpacyEmbeddings distance_model = SpacyEmbeddings ( \"en_core_web_md\" ) model = PolyFuzz ( distance_model ) fit, transform, fit_transform Add fit , transform , and fit_transform in order to use PolyFuzz in production #34 from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] unseen_words = [ \"apple\" , \"apples\" , \"mouse\" ] # Fit model = PolyFuzz ( \"TF-IDF\" ) model . fit ( train_words ) # Transform results = model . transform ( unseen_words ) In the code above, we fit our TF-IDF model on train_words and use .transform() to match the words in unseen_words to the words that we trained on in train_words . After fitting our model, we can save it as follows: model . save ( \"my_model\" ) Then, we can load our model to be used elsewhere: from polyfuzz import PolyFuzz model = PolyFuzz . load ( \"my_model\" ) v0.3.4 \u00b6 Make sure that when you use two lists that are exactly the same, it will return 1 for identical terms: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"house\" ] model = PolyFuzz ( \"TF-IDF\" ) model . match ( from_list , from_list ) This will match each word in from_list to itself and give it a score of 1. Thus, apple will be matched to apple and house will be mapped to house . However, if you input just a single list, it will try to map them within the list without mapping to itself: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" ] model = PolyFuzz ( \"TF-IDF\" ) model . match ( from_list ) In the example above, apple will be mapped to apples and not to apple . Here, we assume that the user wants to find the most similar words within a list without mapping to itself. v0.3.3 \u00b6 Update numpy to \"numpy>=1.20.0\" to prevent this and this issue Update pytorch to \"torch>=1.4.0,<1.7.1\" to prevent save_state_warning error v0.3.2 \u00b6 Fix exploding memory usage when using top_n v0.3.0 \u00b6 Use top_n in polyfuzz.models.TFIDF and polyfuzz.models.Embeddings v0.2.2 \u00b6 Update grouping to include all strings only if identical lists of strings are compared v0.2.0 \u00b6 Update naming convention matcher --> model Update documentation Add basic models to grouper Fix issues with vector order in cosine similarity Update naming of cosine similarity function v0.1.0 \u00b6 Additional tests More thorough documentation Prepare for public release v0.0.1 \u00b6 First release of PolyFuzz Matching through: Edit Distance TF-IDF Embeddings Custom models Grouping of results with custom models Evaluation through precision-recall curves","title":"Releases"},{"location":"releases/#v040","text":"Added new models (SentenceTransformers, Gensim, USE, Spacy) Added .fit , .transform , and .fit_transform methods Added .save and PolyFuzz.load() SentenceTransformers from polyfuzz.models import SentenceEmbeddings distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" ) model = PolyFuzz ( distance_model ) Gensim from polyfuzz.models import GensimEmbeddings distance_model = GensimEmbeddings ( \"glove-twitter-25\" ) model = PolyFuzz ( distance_model ) USE from polyfuzz.models import USEEmbeddings distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) model = PolyFuzz ( distance_model ) Spacy from polyfuzz.models import SpacyEmbeddings distance_model = SpacyEmbeddings ( \"en_core_web_md\" ) model = PolyFuzz ( distance_model ) fit, transform, fit_transform Add fit , transform , and fit_transform in order to use PolyFuzz in production #34 from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] unseen_words = [ \"apple\" , \"apples\" , \"mouse\" ] # Fit model = PolyFuzz ( \"TF-IDF\" ) model . fit ( train_words ) # Transform results = model . transform ( unseen_words ) In the code above, we fit our TF-IDF model on train_words and use .transform() to match the words in unseen_words to the words that we trained on in train_words . After fitting our model, we can save it as follows: model . save ( \"my_model\" ) Then, we can load our model to be used elsewhere: from polyfuzz import PolyFuzz model = PolyFuzz . load ( \"my_model\" )","title":"v0.4.0"},{"location":"releases/#v034","text":"Make sure that when you use two lists that are exactly the same, it will return 1 for identical terms: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"house\" ] model = PolyFuzz ( \"TF-IDF\" ) model . match ( from_list , from_list ) This will match each word in from_list to itself and give it a score of 1. Thus, apple will be matched to apple and house will be mapped to house . However, if you input just a single list, it will try to map them within the list without mapping to itself: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" ] model = PolyFuzz ( \"TF-IDF\" ) model . match ( from_list ) In the example above, apple will be mapped to apples and not to apple . Here, we assume that the user wants to find the most similar words within a list without mapping to itself.","title":"v0.3.4"},{"location":"releases/#v033","text":"Update numpy to \"numpy>=1.20.0\" to prevent this and this issue Update pytorch to \"torch>=1.4.0,<1.7.1\" to prevent save_state_warning error","title":"v0.3.3"},{"location":"releases/#v032","text":"Fix exploding memory usage when using top_n","title":"v0.3.2"},{"location":"releases/#v030","text":"Use top_n in polyfuzz.models.TFIDF and polyfuzz.models.Embeddings","title":"v0.3.0"},{"location":"releases/#v022","text":"Update grouping to include all strings only if identical lists of strings are compared","title":"v0.2.2"},{"location":"releases/#v020","text":"Update naming convention matcher --> model Update documentation Add basic models to grouper Fix issues with vector order in cosine similarity Update naming of cosine similarity function","title":"v0.2.0"},{"location":"releases/#v010","text":"Additional tests More thorough documentation Prepare for public release","title":"v0.1.0"},{"location":"releases/#v001","text":"First release of PolyFuzz Matching through: Edit Distance TF-IDF Embeddings Custom models Grouping of results with custom models Evaluation through precision-recall curves","title":"v0.0.1"},{"location":"api/linkage/","text":"polyfuzz.linkage \u00b6 single_linkage ( matches , min_similarity = 0.8 ) \u00b6 Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns: Type Description clusters The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster Source code in polyfuzz\\linkage.py def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map","title":"Linkage"},{"location":"api/linkage/#polyfuzzlinkage","text":"","title":"polyfuzz.linkage"},{"location":"api/linkage/#polyfuzz.linkage.single_linkage","text":"Single linkage clustering from column 'From' to column 'To' matches contains three columns: From , To , and Similarity where Similarity is already the minimum similarity score and thus no checking for minimum similarity is necessary. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for creating groups required min_similarity float minimum similarity between strings before they can be merged into a group 0.8 Returns: Type Description clusters The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster Source code in polyfuzz\\linkage.py def single_linkage ( matches : pd . DataFrame , min_similarity : float = 0.8 ) -> Tuple [ Mapping [ int , List [ str ]], Mapping [ str , int ], Mapping [ str , str ]]: \"\"\" Single linkage clustering from column 'From' to column 'To' `matches` contains three columns: *From*, *To*, and *Similarity* where *Similarity* is already the minimum similarity score and thus no checking for minimum similarity is necessary. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for creating groups min_similarity: minimum similarity between strings before they can be merged into a group Returns: clusters: The populated clusters cluster_mapping: The mapping from a string to a cluster cluster_name_map: The mapping from a string to the representative string in its respective cluster \"\"\" matches = matches . loc [ matches . Similarity > min_similarity , :] cluster_mapping = {} cluster_id = 0 for row in matches . itertuples (): # If from string has not already been mapped if not cluster_mapping . get ( row . From ): # If the to string has not already been mapped if not cluster_mapping . get ( row . To ): cluster_mapping [ row . To ] = cluster_id cluster_mapping [ row . From ] = cluster_id cluster_id += 1 # If the to string has already been mapped else : cluster_mapping [ row . From ] = cluster_mapping . get ( row . To ) # Populate the clusters clusters = {} for key , value in cluster_mapping . items (): clusters . setdefault ( value , []) clusters [ value ] . append ( key ) cluster_name_map = { key : clusters . get ( value )[ 0 ] for key , value in cluster_mapping . items ()} return clusters , cluster_mapping , cluster_name_map","title":"single_linkage()"},{"location":"api/metrics/","text":"polyfuzz.metrics \u00b6 precision_recall_curve ( matches , precision_steps = 0.01 ) \u00b6 Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns: Type Description min_precisions minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step Source code in polyfuzz\\metrics.py def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision visualize_precision_recall ( matches , min_precisions , recall , kde = True , save_path = None ) \u00b6 Visualize the precision recall curve for one or more models Parameters: Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall ( matches , min_precisions , recall , save_path = \"data/results.png\" ) Source code in polyfuzz\\metrics.py def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = .1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , .61 , .7 , .902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 )","title":"Metrics"},{"location":"api/metrics/#polyfuzzmetrics","text":"","title":"polyfuzz.metrics"},{"location":"api/metrics/#polyfuzz.metrics.precision_recall_curve","text":"Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default matches DataFrame contains the columns From , To , and Similarity used for calculating precision, recall, and average precision required precision_steps float the incremental steps in minimum precision 0.01 Returns: Type Description min_precisions minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step Source code in polyfuzz\\metrics.py def precision_recall_curve ( matches : pd . DataFrame , precision_steps : float = 0.01 ) -> Tuple [ List [ float ], List [ float ], List [ float ]]: \"\"\" Calculate precision recall curve based on minimum similarity between strings A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision precision_steps: the incremental steps in minimum precision Returns: min_precisions: minimum precision steps recall: recall per minimum precision step average_precision: average precision per minimum precision step \"\"\" min_precisions = list ( np . arange ( 0. , 1 + precision_steps , precision_steps )) average_precision = [] recall = [] similarities = matches . Similarity . values total = len ( matches ) for min_precision in min_precisions : selection = similarities [ similarities >= min_precision ] recall . append ( len ( selection ) / total ) with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = RuntimeWarning ) average_precision . append ( float ( np . mean ( selection ))) return min_precisions , recall , average_precision","title":"precision_recall_curve()"},{"location":"api/metrics/#polyfuzz.metrics.visualize_precision_recall","text":"Visualize the precision recall curve for one or more models Parameters: Name Type Description Default matches Mapping[str, pandas.core.frame.DataFrame] contains the columns From , To , and Similarity used for calculating precision, recall, and average precision per model required min_precisions Mapping[str, List[float]] minimum precision steps per model required recall Mapping[str, List[float]] recall per minimum precision step per model required kde bool whether to also visualize the kde plot True save_path str the path to save the resulting image to None Usage: visualize_precision_recall ( matches , min_precisions , recall , save_path = \"data/results.png\" ) Source code in polyfuzz\\metrics.py def visualize_precision_recall ( matches : Mapping [ str , pd . DataFrame ], min_precisions : Mapping [ str , List [ float ]], recall : Mapping [ str , List [ float ]], kde : bool = True , save_path : str = None ): \"\"\" Visualize the precision recall curve for one or more models Arguments: matches: contains the columns *From*, *To*, and *Similarity* used for calculating precision, recall, and average precision per model min_precisions: minimum precision steps per model recall: recall per minimum precision step per model kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python visualize_precision_recall(matches, min_precisions, recall, save_path=\"data/results.png\") ``` \"\"\" SMALL_SIZE = 10 MEDIUM_SIZE = 12 BIGGER_SIZE = 14 plt . rc ( 'font' , size = SMALL_SIZE ) # controls default text sizes plt . rc ( 'axes' , titlesize = SMALL_SIZE ) # fontsize of the axes title plt . rc ( 'axes' , labelsize = MEDIUM_SIZE ) # fontsize of the x and y labels plt . rc ( 'xtick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'ytick' , labelsize = SMALL_SIZE ) # fontsize of the tick labels plt . rc ( 'legend' , fontsize = SMALL_SIZE ) # legend fontsize plt . rc ( 'figure' , titlesize = BIGGER_SIZE ) # fontsize of the figure title if not isinstance ( matches , dict ): matches = { \"Model\" : matches } min_precisions = { \"Model\" : min_precisions } recall = { \"Model\" : recall } # Create single dataset of similarity score for all models distribution_data = [( matches [ name ] . Similarity . values , [ name for _ in range ( len ( matches [ name ]))]) for name in matches . keys ()] distribution_data = pd . DataFrame ( np . hstack ( distribution_data ) . T , columns = [ \"Similarity\" , \"Model\" ]) distribution_data . Similarity = distribution_data . Similarity . astype ( float ) model_names = list ( matches . keys ()) # Create layout cmap = get_cmap ( 'Accent' ) fig = plt . figure ( figsize = ( 20 , 5 )) if len ( model_names ) == 1 : middle = 0 else : middle = .1 if kde : widths = [ 1.5 , middle , 1.5 ] else : widths = [ 1.5 , middle , 0 ] heights = [ 1.5 ] gs = gridspec . GridSpec ( 1 , 3 , width_ratios = widths , height_ratios = heights ) ax1 = plt . subplot ( gs [:, 0 ]) if kde : ax2 = plt . subplot ( gs [:, 2 ], sharex = ax1 ) # Precision-recall curve for color , model_name in zip ( cmap . colors , model_names ): ax1 . plot ( min_precisions [ model_name ], recall [ model_name ], color = color ) ax1 . set_ylim ( bottom = 0 , top = 1 ) ax1 . set_xlim ( left = 0 , right = 1 ) ax1 . spines [ 'right' ] . set_visible ( False ) ax1 . spines [ 'top' ] . set_visible ( False ) ax1 . set_xlabel ( r \"$\\bf {Precision} $\" + \" \\n (Minimum Similarity)\" ) ax1 . set_ylabel ( r \"$\\bf {Recall} $\" + \" \\n (Percentage Matched)\" ) # Similarity Histogram if kde : for color , model_name in zip ( cmap . colors , model_names ): sns . kdeplot ( matches [ model_name ][ \"Similarity\" ], fill = True , ax = ax2 , color = color ) ax2 . yaxis . set_label_position ( \"right\" ) ax2 . yaxis . tick_right () ax2 . set_xlabel ( r \"$\\bf {Similarity} $\" ) ax2 . set_ylabel ( \"\" ) ax2 . set_xlim ( left =- 0 , right = 1 ) plt . setp ([ ax2 ], title = 'Score Frequency - KDE' ) # Titles if len ( model_names ) == 1 and kde : fig . suptitle ( f 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) elif kde : fig . suptitle ( 'Score Metrics' , size = 20 , y = 1 , x = 0.5 ) plt . setp ([ ax1 ], title = 'Precision-Recall Curve' ) else : fig . suptitle ( 'Precision-Recall Curve' , size = 20 , y = 1 , x = 0.45 ) # Custom Legend if len ( model_names ) > 1 : custom_lines = [ Line2D ([ 0 ], [ 0 ], color = color , lw = 4 ) for color , model_name in zip ( cmap . colors , model_names )] ax1 . legend ( custom_lines , model_names , bbox_to_anchor = ( 1.05 , .61 , .7 , .902 ), loc = 3 , ncol = 1 , borderaxespad = 0. , frameon = True , fontsize = 10 ) if save_path : plt . savefig ( save_path , dpi = 300 )","title":"visualize_precision_recall()"},{"location":"api/polyfuzz/","text":"polyfuzz.polyfuzz.PolyFuzz \u00b6 PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters: Name Type Description Default method Union[str, polyfuzz.models._base.BaseMatcher, List[polyfuzz.models._base.BaseMatcher]] the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. 'TF-IDF' verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" ) If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) model = pf . PolyFuzz ( tfidf ) You can also select multiple models in order to compare performance: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) edit = EditDistance ( n_jobs =- 1 ) model = pf . PolyFuzz ([ tfidf , edit ]) You can use embedding model, like Flair: from flair.embeddings import WordEmbeddings , TransformerWordEmbeddings fasttext_embedding = WordEmbeddings ( 'news' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) embedding = Embeddings ([ fasttext_embedding , bert_embedding ], min_similarity = 0.0 ) model = pf . PolyFuzz ( embedding ) Source code in polyfuzz\\polyfuzz.py class PolyFuzz : \"\"\" PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Arguments: method: the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\") ``` If you want more control over the String Matching models, you can load in these models separately: ```python tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") model = pf.PolyFuzz(tfidf) ``` You can also select multiple models in order to compare performance: ```python tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") edit = EditDistance(n_jobs=-1) model = pf.PolyFuzz([tfidf, edit]) ``` You can use embedding model, like Flair: ```python from flair.embeddings import WordEmbeddings, TransformerWordEmbeddings fasttext_embedding = WordEmbeddings('news') bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased') embedding = Embeddings([fasttext_embedding, bert_embedding ], min_similarity=0.0) model = pf.PolyFuzz(embedding) ``` \"\"\" def __init__ ( self , method : Union [ str , BaseMatcher , List [ BaseMatcher ]] = \"TF-IDF\" , verbose : bool = False ): self . method = method self . matches = None # Metrics self . min_precisions = None self . recalls = None self . average_precisions = None # Cluster self . clusters = None self . cluster_mappings = None self . grouped_matches = None if verbose : logger . setLevel ( logging . DEBUG ) else : logger . setLevel ( logging . WARNING ) def match ( self , from_list : List [ str ], to_list : List [ str ] = None , top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . method = TFIDF ( min_similarity = 0 , top_n = top_n ) self . matches = { \"TF-IDF\" : self . method . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . method = RapidFuzz () self . matches = { \"EditDistance\" : self . method . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . method = Embeddings ( min_similarity = 0 , top_n = top_n ) self . matches = { \"Embeddings\" : self . method . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self def fit ( self , from_list : List [ str ], to_list : List [ str ] = None ): \"\"\" Fit one or model distance models on `from_list` if no `to_list` is given or fit them on `to_list` if both `from_list` and `to_list` are given. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only `from_list` and keep `to_list` empty. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` Now, whenever you apply `.transform(new_list)`, the `new_list` will be mapped to the words in `to_list`. You can also fit on a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"string_three\", \"string_four\"]) ``` \"\"\" self . match ( from_list , to_list ) if to_list is not None : self . to_list = to_list else : self . to_list = from_list return self def transform ( self , from_list : List [ str ]) -> Mapping [ str , pd . DataFrame ]: \"\"\" After fitting your model, match all words in `from_list` to the words that were fitted on previously. Arguments: from_list: The list from which you want mappings. Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"input_string_1\", \"input_string2\"]) ``` Then, you can transform and normalize new strings: ```python results = model.transform([\"input_string_1\", \"input_string2\"]) ``` \"\"\" all_matches = {} if isinstance ( self . method , BaseMatcher ): matches = self . method . match ( from_list , self . to_list , re_train = False ) all_matches [ self . method . type ] = matches elif isinstance ( self . method , Iterable ): for model in self . method : all_matches [ model . type ] = model . match ( from_list , self . to_list , re_train = False ) return all_matches def fit_transform ( self , from_list : List [ str ], to_list : List [ str ] = None ) -> Mapping [ str , pd . DataFrame ]: \"\"\" Fit and transform lists of words on one or more distance models. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can also fit and transform a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform([\"string_three\", \"string_four\"]) ``` \"\"\" self . fit ( from_list , to_list ) return self . transform ( from_list ) def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path ) def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the PolyFuzz file you want to load Usage: ```python PolyFuzz.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : model = joblib . load ( file ) return model def _create_groups ( self , name : str , model : BaseMatcher , link_min_similarity : float , group_all_strings : bool ): \"\"\" Create groups based on either the To mappings if you compare two different lists of strings, or the From mappings if you compare lists of strings that are equal (set group_all_strings to True) \"\"\" if group_all_strings : strings = list ( self . matches [ name ] . From . dropna () . unique ()) else : strings = list ( self . matches [ name ] . To . dropna () . unique ()) # Create clusters matches = model . match ( strings ) clusters , cluster_id_map , cluster_name_map = single_linkage ( matches , link_min_similarity ) # Map the `to` list to groups df = self . matches [ name ] df [ \"Group\" ] = df [ 'To' ] . map ( cluster_name_map ) . fillna ( df [ 'To' ]) self . matches [ name ] = df # Track clusters and their ids self . clusters [ name ] = clusters self . cluster_mappings [ name ] = cluster_id_map def _update_model_ids ( self ): \"\"\" Update model ids such that there is no overlap between ids \"\"\" # Give models a model_id if it didn't already exist for index , model in enumerate ( self . method ): if not model . model_id : model . model_id = f \"Model { index } \" # Update duplicate names model_ids = [ model . model_id for model in self . method ] if len ( set ( model_ids )) != len ( model_ids ): for index , model in enumerate ( self . method ): model . model_id = f \"Model { index } \" fit ( self , from_list , to_list = None ) \u00b6 Fit one or model distance models on from_list if no to_list is given or fit them on to_list if both from_list and to_list are given. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only from_list and keep to_list empty. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) Now, whenever you apply .transform(new_list) , the new_list will be mapped to the words in to_list . You can also fit on a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\polyfuzz.py def fit ( self , from_list : List [ str ], to_list : List [ str ] = None ): \"\"\" Fit one or model distance models on `from_list` if no `to_list` is given or fit them on `to_list` if both `from_list` and `to_list` are given. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only `from_list` and keep `to_list` empty. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` Now, whenever you apply `.transform(new_list)`, the `new_list` will be mapped to the words in `to_list`. You can also fit on a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"string_three\", \"string_four\"]) ``` \"\"\" self . match ( from_list , to_list ) if to_list is not None : self . to_list = to_list else : self . to_list = from_list return self fit_transform ( self , from_list , to_list = None ) \u00b6 Fit and transform lists of words on one or more distance models. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can also fit and transform a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ([ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\polyfuzz.py def fit_transform ( self , from_list : List [ str ], to_list : List [ str ] = None ) -> Mapping [ str , pd . DataFrame ]: \"\"\" Fit and transform lists of words on one or more distance models. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can also fit and transform a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform([\"string_three\", \"string_four\"]) ``` \"\"\" self . fit ( from_list , to_list ) return self . transform ( from_list ) get_cluster_mappings ( self , name = None ) \u00b6 Get the mappings from the To column to its respective column Source code in polyfuzz\\polyfuzz.py def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings get_clusters ( self , model_id = None ) \u00b6 Get the groupings/clusters from a single model Parameters: Name Type Description Default model_id str the model id of the model if you have specified multiple models None Source code in polyfuzz\\polyfuzz.py def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters get_ids ( self ) \u00b6 Get all model ids for easier access Source code in polyfuzz\\polyfuzz.py def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None get_matches ( self , model_id = None ) \u00b6 Get the matches from one or more models Source code in polyfuzz\\polyfuzz.py def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches group ( self , model = None , link_min_similarity = 0.75 , group_all_strings = False ) \u00b6 From the matches, group the To matches together using single linkage Parameters: Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column Source code in polyfuzz\\polyfuzz.py def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) load ( path ) classmethod \u00b6 Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the PolyFuzz file you want to load required Usage: PolyFuzz . load ( \"my_model\" ) Source code in polyfuzz\\polyfuzz.py @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the PolyFuzz file you want to load Usage: ```python PolyFuzz.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : model = joblib . load ( file ) return model match ( self , from_list , to_list = None , top_n = 1 ) \u00b6 Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None top_n int The number of matches you want returned. This is currently only implemented for polyfuzz.models.TFIDF and polyfuzz.models.Embeddings as they can computationally handle more comparisons. 1 Updates self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id. Source code in polyfuzz\\polyfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . method = TFIDF ( min_similarity = 0 , top_n = top_n ) self . matches = { \"TF-IDF\" : self . method . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . method = RapidFuzz () self . matches = { \"EditDistance\" : self . method . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . method = Embeddings ( min_similarity = 0 , top_n = top_n ) self . matches = { \"Embeddings\" : self . method . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self save ( self , path ) \u00b6 Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" ) Source code in polyfuzz\\polyfuzz.py def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) transform ( self , from_list ) \u00b6 After fitting your model, match all words in from_list to the words that were fitted on previously. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. required Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"input_string_1\" , \"input_string2\" ]) Then, you can transform and normalize new strings: results = model . transform ([ \"input_string_1\" , \"input_string2\" ]) Source code in polyfuzz\\polyfuzz.py def transform ( self , from_list : List [ str ]) -> Mapping [ str , pd . DataFrame ]: \"\"\" After fitting your model, match all words in `from_list` to the words that were fitted on previously. Arguments: from_list: The list from which you want mappings. Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"input_string_1\", \"input_string2\"]) ``` Then, you can transform and normalize new strings: ```python results = model.transform([\"input_string_1\", \"input_string2\"]) ``` \"\"\" all_matches = {} if isinstance ( self . method , BaseMatcher ): matches = self . method . match ( from_list , self . to_list , re_train = False ) all_matches [ self . method . type ] = matches elif isinstance ( self . method , Iterable ): for model in self . method : all_matches [ model . type ] = model . match ( from_list , self . to_list , re_train = False ) return all_matches visualize_precision_recall ( self , kde = False , save_path = None ) \u00b6 Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) model . visualize_precision_recall ( save_path = \"results.png\" ) Source code in polyfuzz\\polyfuzz.py def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path )","title":"PolyFuzz"},{"location":"api/polyfuzz/#polyfuzzpolyfuzzpolyfuzz","text":"PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Parameters: Name Type Description Default method Union[str, polyfuzz.models._base.BaseMatcher, List[polyfuzz.models._base.BaseMatcher]] the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. 'TF-IDF' verbose bool Changes the verbosity of the model, Set to True if you want to track the stages of the model. False Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" ) If you want more control over the String Matching models, you can load in these models separately: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) model = pf . PolyFuzz ( tfidf ) You can also select multiple models in order to compare performance: tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF-Sklearn\" ) edit = EditDistance ( n_jobs =- 1 ) model = pf . PolyFuzz ([ tfidf , edit ]) You can use embedding model, like Flair: from flair.embeddings import WordEmbeddings , TransformerWordEmbeddings fasttext_embedding = WordEmbeddings ( 'news' ) bert_embedding = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) embedding = Embeddings ([ fasttext_embedding , bert_embedding ], min_similarity = 0.0 ) model = pf . PolyFuzz ( embedding ) Source code in polyfuzz\\polyfuzz.py class PolyFuzz : \"\"\" PolyFuzz class for Fuzzy string matching, grouping, and evaluation. Arguments: method: the method(s) used for matching. For quick selection of models select one of the following: \"EditDistance\", \"TF-IDF\" or \"Embeddings\". If you want more control over the models above, pass in a model from polyfuzz.models. For examples, see usage below. verbose: Changes the verbosity of the model, Set to True if you want to track the stages of the model. Usage: For basic, out-of-the-box usage, run the code below. You can replace \"TF-IDF\" with either \"EditDistance\" or \"Embeddings\" for quick access to these models: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\") ``` If you want more control over the String Matching models, you can load in these models separately: ```python tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") model = pf.PolyFuzz(tfidf) ``` You can also select multiple models in order to compare performance: ```python tfidf = TFIDF(n_gram_range=(3, 3), min_similarity=0, model_id=\"TF-IDF-Sklearn\") edit = EditDistance(n_jobs=-1) model = pf.PolyFuzz([tfidf, edit]) ``` You can use embedding model, like Flair: ```python from flair.embeddings import WordEmbeddings, TransformerWordEmbeddings fasttext_embedding = WordEmbeddings('news') bert_embedding = TransformerWordEmbeddings('bert-base-multilingual-cased') embedding = Embeddings([fasttext_embedding, bert_embedding ], min_similarity=0.0) model = pf.PolyFuzz(embedding) ``` \"\"\" def __init__ ( self , method : Union [ str , BaseMatcher , List [ BaseMatcher ]] = \"TF-IDF\" , verbose : bool = False ): self . method = method self . matches = None # Metrics self . min_precisions = None self . recalls = None self . average_precisions = None # Cluster self . clusters = None self . cluster_mappings = None self . grouped_matches = None if verbose : logger . setLevel ( logging . DEBUG ) else : logger . setLevel ( logging . WARNING ) def match ( self , from_list : List [ str ], to_list : List [ str ] = None , top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . method = TFIDF ( min_similarity = 0 , top_n = top_n ) self . matches = { \"TF-IDF\" : self . method . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . method = RapidFuzz () self . matches = { \"EditDistance\" : self . method . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . method = Embeddings ( min_similarity = 0 , top_n = top_n ) self . matches = { \"Embeddings\" : self . method . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self def fit ( self , from_list : List [ str ], to_list : List [ str ] = None ): \"\"\" Fit one or model distance models on `from_list` if no `to_list` is given or fit them on `to_list` if both `from_list` and `to_list` are given. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only `from_list` and keep `to_list` empty. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` Now, whenever you apply `.transform(new_list)`, the `new_list` will be mapped to the words in `to_list`. You can also fit on a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"string_three\", \"string_four\"]) ``` \"\"\" self . match ( from_list , to_list ) if to_list is not None : self . to_list = to_list else : self . to_list = from_list return self def transform ( self , from_list : List [ str ]) -> Mapping [ str , pd . DataFrame ]: \"\"\" After fitting your model, match all words in `from_list` to the words that were fitted on previously. Arguments: from_list: The list from which you want mappings. Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"input_string_1\", \"input_string2\"]) ``` Then, you can transform and normalize new strings: ```python results = model.transform([\"input_string_1\", \"input_string2\"]) ``` \"\"\" all_matches = {} if isinstance ( self . method , BaseMatcher ): matches = self . method . match ( from_list , self . to_list , re_train = False ) all_matches [ self . method . type ] = matches elif isinstance ( self . method , Iterable ): for model in self . method : all_matches [ model . type ] = model . match ( from_list , self . to_list , re_train = False ) return all_matches def fit_transform ( self , from_list : List [ str ], to_list : List [ str ] = None ) -> Mapping [ str , pd . DataFrame ]: \"\"\" Fit and transform lists of words on one or more distance models. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can also fit and transform a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform([\"string_three\", \"string_four\"]) ``` \"\"\" self . fit ( from_list , to_list ) return self . transform ( from_list ) def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path ) def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings ) def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file ) @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the PolyFuzz file you want to load Usage: ```python PolyFuzz.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : model = joblib . load ( file ) return model def _create_groups ( self , name : str , model : BaseMatcher , link_min_similarity : float , group_all_strings : bool ): \"\"\" Create groups based on either the To mappings if you compare two different lists of strings, or the From mappings if you compare lists of strings that are equal (set group_all_strings to True) \"\"\" if group_all_strings : strings = list ( self . matches [ name ] . From . dropna () . unique ()) else : strings = list ( self . matches [ name ] . To . dropna () . unique ()) # Create clusters matches = model . match ( strings ) clusters , cluster_id_map , cluster_name_map = single_linkage ( matches , link_min_similarity ) # Map the `to` list to groups df = self . matches [ name ] df [ \"Group\" ] = df [ 'To' ] . map ( cluster_name_map ) . fillna ( df [ 'To' ]) self . matches [ name ] = df # Track clusters and their ids self . clusters [ name ] = clusters self . cluster_mappings [ name ] = cluster_id_map def _update_model_ids ( self ): \"\"\" Update model ids such that there is no overlap between ids \"\"\" # Give models a model_id if it didn't already exist for index , model in enumerate ( self . method ): if not model . model_id : model . model_id = f \"Model { index } \" # Update duplicate names model_ids = [ model . model_id for model in self . method ] if len ( set ( model_ids )) != len ( model_ids ): for index , model in enumerate ( self . method ): model . model_id = f \"Model { index } \"","title":"polyfuzz.polyfuzz.PolyFuzz"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.fit","text":"Fit one or model distance models on from_list if no to_list is given or fit them on to_list if both from_list and to_list are given. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only from_list and keep to_list empty. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) Now, whenever you apply .transform(new_list) , the new_list will be mapped to the words in to_list . You can also fit on a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\polyfuzz.py def fit ( self , from_list : List [ str ], to_list : List [ str ] = None ): \"\"\" Fit one or model distance models on `from_list` if no `to_list` is given or fit them on `to_list` if both `from_list` and `to_list` are given. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Thus, if you want to train on a single list instead, use only `from_list` and keep `to_list` empty. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` Now, whenever you apply `.transform(new_list)`, the `new_list` will be mapped to the words in `to_list`. You can also fit on a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"string_three\", \"string_four\"]) ``` \"\"\" self . match ( from_list , to_list ) if to_list is not None : self . to_list = to_list else : self . to_list = from_list return self","title":"fit()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.fit_transform","text":"Fit and transform lists of words on one or more distance models. Typically, the to_list will be tracked as the list that we want to transform our from_list to. In other words, it is the golden list of words that we want the words in the from_list mapped to. However, you can also choose a single from_list and leave to_list empty to map all words from within from_list to each other. Then, from_list will be tracked instead as the golden list of words. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can also fit and transform a single list of words: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) results = model . fit_transform ([ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\polyfuzz.py def fit_transform ( self , from_list : List [ str ], to_list : List [ str ] = None ) -> Mapping [ str , pd . DataFrame ]: \"\"\" Fit and transform lists of words on one or more distance models. Typically, the `to_list` will be tracked as the list that we want to transform our `from_list` to. In other words, it is the golden list of words that we want the words in the `from_list` mapped to. However, you can also choose a single `from_list` and leave `to_list` empty to map all words from within `from_list` to each other. Then, `from_list` will be tracked instead as the golden list of words. Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can also fit and transform a single list of words: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") results = model.fit_transform([\"string_three\", \"string_four\"]) ``` \"\"\" self . fit ( from_list , to_list ) return self . transform ( from_list )","title":"fit_transform()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_cluster_mappings","text":"Get the mappings from the To column to its respective column Source code in polyfuzz\\polyfuzz.py def get_cluster_mappings ( self , name : str = None ) -> Mapping [ str , int ]: \"\"\" Get the mappings from the `To` column to its respective column \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . cluster_mappings . values ())[ 0 ] elif len ( self . matches ) > 1 and name : return self . cluster_mappings [ name ] return self . cluster_mappings","title":"get_cluster_mappings()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_clusters","text":"Get the groupings/clusters from a single model Parameters: Name Type Description Default model_id str the model id of the model if you have specified multiple models None Source code in polyfuzz\\polyfuzz.py def get_clusters ( self , model_id : str = None ) -> Mapping [ str , List [ str ]]: \"\"\" Get the groupings/clusters from a single model Arguments: model_id: the model id of the model if you have specified multiple models \"\"\" check_matches ( self ) check_grouped ( self ) if len ( self . matches ) == 1 : return list ( self . clusters . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . clusters [ model_id ] return self . clusters","title":"get_clusters()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_ids","text":"Get all model ids for easier access Source code in polyfuzz\\polyfuzz.py def get_ids ( self ) -> Union [ str , List [ str ], None ]: \"\"\" Get all model ids for easier access \"\"\" check_matches ( self ) if isinstance ( self . method , str ): return self . method elif isinstance ( self . method , Iterable ): return [ model . model_id for model in self . method ] return None","title":"get_ids()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.get_matches","text":"Get the matches from one or more models Source code in polyfuzz\\polyfuzz.py def get_matches ( self , model_id : str = None ) -> Union [ pd . DataFrame , Mapping [ str , pd . DataFrame ]]: \"\"\" Get the matches from one or more models\"\"\" check_matches ( self ) if len ( self . matches ) == 1 : return list ( self . matches . values ())[ 0 ] elif len ( self . matches ) > 1 and model_id : return self . matches [ model_id ] return self . matches","title":"get_matches()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.group","text":"From the matches, group the To matches together using single linkage Parameters: Name Type Description Default model Union[str, polyfuzz.models._base.BaseMatcher] you can choose one of the models in polyfuzz.models to be used as a grouper None link_min_similarity float the minimum similarity between strings before they are grouped in a single linkage fashion 0.75 group_all_strings bool if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. False Updates: self.matches: Adds a column Group that is the grouped version of the To column Source code in polyfuzz\\polyfuzz.py def group ( self , model : Union [ str , BaseMatcher ] = None , link_min_similarity : float = 0.75 , group_all_strings : bool = False ): \"\"\" From the matches, group the `To` matches together using single linkage Arguments: model: you can choose one of the models in `polyfuzz.models` to be used as a grouper link_min_similarity: the minimum similarity between strings before they are grouped in a single linkage fashion group_all_strings: if you want to compare a list of strings with itself and then cluster those strings, set this to True. Otherwise, only the strings that were mapped To are clustered. Updates: self.matches: Adds a column `Group` that is the grouped version of the `To` column \"\"\" check_matches ( self ) self . clusters = {} self . cluster_mappings = {} # Standard models - quick access if isinstance ( model , str ): if model in [ \"TF-IDF\" , \"TFIDF\" ]: model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: model = RapidFuzz () elif self . method in [ \"Embeddings\" , \"Embedding\" ]: model = Embeddings ( min_similarity = link_min_similarity ) else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" \"* Or None if you want to automatically use TF-IDF\" ) # Use TF-IDF if no model is specified elif not model : model = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = link_min_similarity ) # Group per model for name , match in self . matches . items (): self . _create_groups ( name , model , link_min_similarity , group_all_strings )","title":"group()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.load","text":"Loads the model from the specified path Parameters: Name Type Description Default path str the location and name of the PolyFuzz file you want to load required Usage: PolyFuzz . load ( \"my_model\" ) Source code in polyfuzz\\polyfuzz.py @classmethod def load ( cls , path : str ): \"\"\" Loads the model from the specified path Arguments: path: the location and name of the PolyFuzz file you want to load Usage: ```python PolyFuzz.load(\"my_model\") ``` \"\"\" with open ( path , 'rb' ) as file : model = joblib . load ( file ) return model","title":"load()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.match","text":"Match the from_list of strings to the to_list of strings with whatever models you have initialized Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the from_list and ignore the to_list . required to_list List[str] The list where you want to map to None top_n int The number of matches you want returned. This is currently only implemented for polyfuzz.models.TFIDF and polyfuzz.models.Embeddings as they can computationally handle more comparisons. 1 Updates self.matches: A dictionary with the matches from all models, can be accessed with model.get_all_matches or model.get_match(\"TF-IDF\") Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) You can access the results matches with model.get_all_matches or a specific model with model.get_match(\"TF-IDF\") based on their model_id. Source code in polyfuzz\\polyfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , top_n : int = 1 ): \"\"\" Match the from_list of strings to the to_list of strings with whatever models you have initialized Arguments: from_list: The list from which you want mappings. If you want to map items within a list, and not map the items to themselves, you can supply only the `from_list` and ignore the `to_list`. to_list: The list where you want to map to top_n: The number of matches you want returned. This is currently only implemented for `polyfuzz.models.TFIDF` and `polyfuzz.models.Embeddings` as they can computationally handle more comparisons. Updates: self.matches: A dictionary with the matches from all models, can be accessed with `model.get_all_matches` or `model.get_match(\"TF-IDF\")` Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) ``` You can access the results matches with `model.get_all_matches` or a specific model with `model.get_match(\"TF-IDF\")` based on their model_id. \"\"\" # Standard models - quick access if isinstance ( self . method , str ): if self . method in [ \"TF-IDF\" , \"TFIDF\" ]: self . method = TFIDF ( min_similarity = 0 , top_n = top_n ) self . matches = { \"TF-IDF\" : self . method . match ( from_list , to_list )} elif self . method in [ \"EditDistance\" , \"Edit Distance\" ]: self . method = RapidFuzz () self . matches = { \"EditDistance\" : self . method . match ( from_list , to_list )} elif self . method in [ \"Embeddings\" , \"Embedding\" ]: self . method = Embeddings ( min_similarity = 0 , top_n = top_n ) self . matches = { \"Embeddings\" : self . method . match ( from_list , to_list )} else : raise ValueError ( \"Please instantiate the model with one of the following methods: \\n \" \"* 'TF-IDF' \\n \" \"* 'EditDistance' \\n \" \"* 'Embeddings' \\n \" ) logger . info ( f \"Ran model with model id = { self . method } \" ) # Custom models elif isinstance ( self . method , BaseMatcher ): self . matches = { self . method . model_id : self . method . match ( from_list , to_list )} logging . info ( f \"Ran model with model id = { self . method . model_id } \" ) # Multiple custom models elif isinstance ( self . method , Iterable ): self . _update_model_ids () self . matches = {} for model in self . method : self . matches [ model . model_id ] = model . match ( from_list , to_list ) logging . info ( f \"Ran model with model id = { model . model_id } \" ) return self","title":"match()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.save","text":"Saves the model to the specified path Parameters: Name Type Description Default path str the location and name of the file you want to save required Usage: model . save ( \"my_model\" ) Source code in polyfuzz\\polyfuzz.py def save ( self , path : str ) -> None : \"\"\" Saves the model to the specified path Arguments: path: the location and name of the file you want to save Usage: ```python model.save(\"my_model\") ``` \"\"\" with open ( path , 'wb' ) as file : joblib . dump ( self , file )","title":"save()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.transform","text":"After fitting your model, match all words in from_list to the words that were fitted on previously. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings. required Usage: After having initialized your models, you can pass through lists of strings: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . fit ([ \"input_string_1\" , \"input_string2\" ]) Then, you can transform and normalize new strings: results = model . transform ([ \"input_string_1\" , \"input_string2\" ]) Source code in polyfuzz\\polyfuzz.py def transform ( self , from_list : List [ str ]) -> Mapping [ str , pd . DataFrame ]: \"\"\" After fitting your model, match all words in `from_list` to the words that were fitted on previously. Arguments: from_list: The list from which you want mappings. Usage: After having initialized your models, you can pass through lists of strings: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.fit([\"input_string_1\", \"input_string2\"]) ``` Then, you can transform and normalize new strings: ```python results = model.transform([\"input_string_1\", \"input_string2\"]) ``` \"\"\" all_matches = {} if isinstance ( self . method , BaseMatcher ): matches = self . method . match ( from_list , self . to_list , re_train = False ) all_matches [ self . method . type ] = matches elif isinstance ( self . method , Iterable ): for model in self . method : all_matches [ model . type ] = model . match ( from_list , self . to_list , re_train = False ) return all_matches","title":"transform()"},{"location":"api/polyfuzz/#polyfuzz.polyfuzz.PolyFuzz.visualize_precision_recall","text":"Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as precision since it shows you how precise we believe the matches are at a minimum. Recall can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Parameters: Name Type Description Default kde bool whether to also visualize the kde plot False save_path str the path to save the resulting image to None Usage: import polyfuzz as pf model = pf . PolyFuzz ( \"TF-IDF\" , model_id = \"TF-IDF\" ) model . match ( from_list = [ \"string_one\" , \"string_two\" ], to_list = [ \"string_three\" , \"string_four\" ]) model . visualize_precision_recall ( save_path = \"results.png\" ) Source code in polyfuzz\\polyfuzz.py def visualize_precision_recall ( self , kde : bool = False , save_path : str = None ): \"\"\" Calculate and visualize precision-recall curves A minimum similarity score might be used to identify when a match could be considered to be correct. For example, we can assume that if a similarity score pass 0.95 we are quite confident that the matches are correct. This minimum similarity score can be defined as **precision** since it shows you how precise we believe the matches are at a minimum. **Recall** can then be defined as as the percentage of matches found at a certain minimum similarity score. A high recall means that for a certain minimum precision score, we find many matches. Arguments: kde: whether to also visualize the kde plot save_path: the path to save the resulting image to Usage: ```python import polyfuzz as pf model = pf.PolyFuzz(\"TF-IDF\", model_id=\"TF-IDF\") model.match(from_list = [\"string_one\", \"string_two\"], to_list = [\"string_three\", \"string_four\"]) model.visualize_precision_recall(save_path=\"results.png\") ``` \"\"\" check_matches ( self ) self . min_precisions = {} self . recalls = {} self . average_precisions = {} for name , match in self . matches . items (): min_precision , recall , average_precision = precision_recall_curve ( match ) self . min_precisions [ name ] = min_precision self . recalls [ name ] = recall self . average_precisions [ name ] = average_precision visualize_precision_recall ( self . matches , self . min_precisions , self . recalls , kde , save_path )","title":"visualize_precision_recall()"},{"location":"api/models/base/","text":"polyfuzz.models.BaseMatcher \u00b6 The abstract BaseMatching to be modelled after for string matching Source code in polyfuzz\\models\\_base.py class BaseMatcher ( ABC ): \"\"\" The abstract BaseMatching to be modelled after for string matching \"\"\" def __init__ ( self , model_id : str = \"Model 0\" ): self . model_id = model_id self . type = \"Base Model\" @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError () match ( self , from_list , to_list = None , ** kwargs ) \u00b6 Make sure you follow the same argument structure: Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns: Type Description matches The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" Source code in polyfuzz\\models\\_base.py @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError ()","title":"BaseMatcher"},{"location":"api/models/base/#polyfuzzmodelsbasematcher","text":"The abstract BaseMatching to be modelled after for string matching Source code in polyfuzz\\models\\_base.py class BaseMatcher ( ABC ): \"\"\" The abstract BaseMatching to be modelled after for string matching \"\"\" def __init__ ( self , model_id : str = \"Model 0\" ): self . model_id = model_id self . type = \"Base Model\" @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError ()","title":"polyfuzz.models.BaseMatcher"},{"location":"api/models/base/#polyfuzz.models._base.BaseMatcher.match","text":"Make sure you follow the same argument structure: Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns: Type Description matches The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" Source code in polyfuzz\\models\\_base.py @abstractmethod def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Make sure you follow the same argument structure: Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Columns: * \"From\" * \"To\" * \"Similarity\" \"\"\" raise NotImplementedError ()","title":"match()"},{"location":"api/models/distance/","text":"polyfuzz.models.EditDistance \u00b6 Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters: Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 scorer Callable The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") <cyfunction ratio at 0x0000020BAAB37BA0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , scorer = fuzz . WRatio ) Source code in polyfuzz\\models\\_distance.py class EditDistance ( BaseMatcher ): \"\"\" Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Arguments: n_jobs: Nr of parallel processes, use -1 to use all cores scorer: The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") model_id: The name of the particular instance, used when comparing models Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, scorer=fuzz.WRatio) ``` \"\"\" def __init__ ( self , n_jobs : int = 1 , scorer : Callable = fuzz . ratio , model_id : str = None , normalize : bool = True ): super () . __init__ ( model_id ) self . type = \"EditDistance\" self . scorer = scorer self . normalize = normalize self . equal_lists = False if n_jobs == - 1 : self . n_jobs = cpu_count () else : self . n_jobs = n_jobs def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches def _calculate_edit_distance ( self , from_string : str , to_list : List [ str ]) -> Tuple [ str , str , float ]: \"\"\" Calculate the edit distance between a string and a list \"\"\" if self . equal_lists : to_list . remove ( from_string ) matches = [ self . scorer ( from_string , to_string ) for to_string in to_list ] index = np . argmax ( matches ) value = np . max ( matches ) return from_string , to_list [ index ], value match ( self , from_list , to_list = None , ** kwargs ) \u00b6 Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns: Type Description matches The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_distance.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches","title":"EditDistance"},{"location":"api/models/distance/#polyfuzzmodelseditdistance","text":"Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Parameters: Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 scorer Callable The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") <cyfunction ratio at 0x0000020BAAB37BA0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , scorer = fuzz . WRatio ) Source code in polyfuzz\\models\\_distance.py class EditDistance ( BaseMatcher ): \"\"\" Calculate the Edit Distance between lists of strings using any distance/similarity based scorer Arguments: n_jobs: Nr of parallel processes, use -1 to use all cores scorer: The scorer function to be used to calculate the edit distance. This function should give back a float between 0 and 1, and work as follows: scorer(\"string_one\", \"string_two\") model_id: The name of the particular instance, used when comparing models Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, scorer=fuzz.WRatio) ``` \"\"\" def __init__ ( self , n_jobs : int = 1 , scorer : Callable = fuzz . ratio , model_id : str = None , normalize : bool = True ): super () . __init__ ( model_id ) self . type = \"EditDistance\" self . scorer = scorer self . normalize = normalize self . equal_lists = False if n_jobs == - 1 : self . n_jobs = cpu_count () else : self . n_jobs = n_jobs def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches def _calculate_edit_distance ( self , from_string : str , to_list : List [ str ]) -> Tuple [ str , str , float ]: \"\"\" Calculate the edit distance between a string and a list \"\"\" if self . equal_lists : to_list . remove ( from_string ) matches = [ self . scorer ( from_string , to_string ) for to_string in to_list ] index = np . argmax ( matches ) value = np . max ( matches ) return from_string , to_list [ index ], value","title":"polyfuzz.models.EditDistance"},{"location":"api/models/distance/#polyfuzz.models._distance.EditDistance.match","text":"Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns: Type Description matches The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = EditDistance ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_distance.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = EditDistance(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) if self . normalize : matches [ \"Similarity\" ] = ( matches [ \"Similarity\" ] - matches [ \"Similarity\" ] . min ()) / ( matches [ \"Similarity\" ] . max () - matches [ \"Similarity\" ] . min ()) return matches","title":"match()"},{"location":"api/models/embeddings/","text":"polyfuzz.models.Embeddings \u00b6","title":"Embeddings"},{"location":"api/models/embeddings/#polyfuzzmodelsembeddings","text":"","title":"polyfuzz.models.Embeddings"},{"location":"api/models/gensim/","text":"polyfuzz.models.GensimEmbeddings \u00b6","title":"Gensim"},{"location":"api/models/gensim/#polyfuzzmodelsgensimembeddings","text":"","title":"polyfuzz.models.GensimEmbeddings"},{"location":"api/models/matches/","text":"polyfuzz.models.cosine_similarity \u00b6 Calculate similarity between two matrices/vectors and return best matches Parameters: Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns: Type Description matches The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices , similarity = extract_best_matches ( from_vector , to_vector , method = \"sparse\" ) Source code in polyfuzz\\models\\_utils.py def cosine_similarity ( from_vector : np . ndarray , to_vector : np . ndarray , from_list : List [ str ], to_list : List [ str ], min_similarity : float = 0.75 , top_n : int = 1 , method : str = \"sparse\" ) -> pd . DataFrame : \"\"\" Calculate similarity between two matrices/vectors and return best matches Arguments: from_vector: the matrix or vector representing the embedded strings to map from to_vector: the matrix or vector representing the embedded strings to map to from_list: The list from which you want mappings to_list: The list where you want to map to min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned method: The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory Returns: matches: The best matches between the lists of strings Usage: Make sure to fill the `to_vector` and `from_vector` with vector representations of `to_list` and `from_list` respectively: ```python from polyfuzz.models import extract_best_matches indices, similarity = extract_best_matches(from_vector, to_vector, method=\"sparse\") ``` \"\"\" if to_list is not None : if top_n > len ( set ( to_list )): top_n = len ( set ( to_list )) # Slower but uses less memory if method == \"knn\" : if to_list is None : knn = NearestNeighbors ( n_neighbors = top_n + 1 , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) distances = distances [:, 1 :] indices = indices [:, 1 :] else : knn = NearestNeighbors ( n_neighbors = top_n , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) similarities = [ np . round ( 1 - distances [:, i ], 3 ) for i in range ( distances . shape [ 1 ])] # Fast, but does has some installation issues elif _HAVE_SPARSE_DOT and method == \"sparse\" : if isinstance ( to_vector , np . ndarray ): to_vector = csr_matrix ( to_vector ) if isinstance ( from_vector , np . ndarray ): from_vector = csr_matrix ( from_vector ) # There is a bug with awesome_cossim_topn that when to_vector and from_vector # have the same shape, setting topn to 1 does not work. Apparently, you need # to it at least to 2 for it to work similarity_matrix = awesome_cossim_topn ( from_vector , to_vector . T , top_n + 1 , min_similarity ) if to_list is None : similarity_matrix = similarity_matrix . tolil () similarity_matrix . setdiag ( 0. ) similarity_matrix = similarity_matrix . tocsr () indices = _top_n_idx_sparse ( similarity_matrix , top_n ) similarities = _top_n_similarities_sparse ( similarity_matrix , indices ) indices = np . array ( np . nan_to_num ( np . array ( indices , dtype = np . float ), nan = 0 ), dtype = np . int ) # Faster than knn and slower than sparse but uses more memory else : similarity_matrix = scikit_cosine_similarity ( from_vector , to_vector ) if to_list is None : np . fill_diagonal ( similarity_matrix , 0 ) indices = np . flip ( np . argsort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = np . flip ( np . sort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = [ np . round ( similarities [:, i ], 3 ) for i in range ( similarities . shape [ 1 ])] # Convert results to df if to_list is None : to_list = from_list . copy () columns = ([ \"From\" ] + [ \"To\" if i == 0 else f \"To_ { i + 1 } \" for i in range ( top_n )] + [ \"Similarity\" if i == 0 else f \"Similarity_ { i + 1 } \" for i in range ( top_n )]) matches = [[ to_list [ idx ] for idx in indices [:, i ]] for i in range ( indices . shape [ 1 ])] matches = pd . DataFrame ( np . vstack (([ from_list ], matches , similarities )) . T , columns = columns ) # Update column order columns = [[ \"From\" , \"To\" , \"Similarity\" ]] + [[ f \"To_ { i + 2 } \" , f \"Similarity_ { i + 2 } \" ] for i in range (( top_n - 1 ))] matches = matches . loc [:, [ title for column in columns for title in column ]] # Update types for column in matches . columns : if \"Similarity\" in column : matches [ column ] = matches [ column ] . astype ( float ) matches . loc [ matches [ column ] < 0.001 , column ] = float ( 0 ) matches . loc [ matches [ column ] < 0.001 , column . replace ( \"Similarity\" , \"To\" )] = None return matches","title":"CosineSimilarity"},{"location":"api/models/matches/#polyfuzzmodelscosine_similarity","text":"Calculate similarity between two matrices/vectors and return best matches Parameters: Name Type Description Default from_vector ndarray the matrix or vector representing the embedded strings to map from required to_vector ndarray the matrix or vector representing the embedded strings to map to required from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to required min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' Returns: Type Description matches The best matches between the lists of strings Usage: Make sure to fill the to_vector and from_vector with vector representations of to_list and from_list respectively: from polyfuzz.models import extract_best_matches indices , similarity = extract_best_matches ( from_vector , to_vector , method = \"sparse\" ) Source code in polyfuzz\\models\\_utils.py def cosine_similarity ( from_vector : np . ndarray , to_vector : np . ndarray , from_list : List [ str ], to_list : List [ str ], min_similarity : float = 0.75 , top_n : int = 1 , method : str = \"sparse\" ) -> pd . DataFrame : \"\"\" Calculate similarity between two matrices/vectors and return best matches Arguments: from_vector: the matrix or vector representing the embedded strings to map from to_vector: the matrix or vector representing the embedded strings to map to from_list: The list from which you want mappings to_list: The list where you want to map to min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned method: The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory Returns: matches: The best matches between the lists of strings Usage: Make sure to fill the `to_vector` and `from_vector` with vector representations of `to_list` and `from_list` respectively: ```python from polyfuzz.models import extract_best_matches indices, similarity = extract_best_matches(from_vector, to_vector, method=\"sparse\") ``` \"\"\" if to_list is not None : if top_n > len ( set ( to_list )): top_n = len ( set ( to_list )) # Slower but uses less memory if method == \"knn\" : if to_list is None : knn = NearestNeighbors ( n_neighbors = top_n + 1 , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) distances = distances [:, 1 :] indices = indices [:, 1 :] else : knn = NearestNeighbors ( n_neighbors = top_n , n_jobs =- 1 , metric = 'cosine' ) . fit ( to_vector ) distances , indices = knn . kneighbors ( from_vector ) similarities = [ np . round ( 1 - distances [:, i ], 3 ) for i in range ( distances . shape [ 1 ])] # Fast, but does has some installation issues elif _HAVE_SPARSE_DOT and method == \"sparse\" : if isinstance ( to_vector , np . ndarray ): to_vector = csr_matrix ( to_vector ) if isinstance ( from_vector , np . ndarray ): from_vector = csr_matrix ( from_vector ) # There is a bug with awesome_cossim_topn that when to_vector and from_vector # have the same shape, setting topn to 1 does not work. Apparently, you need # to it at least to 2 for it to work similarity_matrix = awesome_cossim_topn ( from_vector , to_vector . T , top_n + 1 , min_similarity ) if to_list is None : similarity_matrix = similarity_matrix . tolil () similarity_matrix . setdiag ( 0. ) similarity_matrix = similarity_matrix . tocsr () indices = _top_n_idx_sparse ( similarity_matrix , top_n ) similarities = _top_n_similarities_sparse ( similarity_matrix , indices ) indices = np . array ( np . nan_to_num ( np . array ( indices , dtype = np . float ), nan = 0 ), dtype = np . int ) # Faster than knn and slower than sparse but uses more memory else : similarity_matrix = scikit_cosine_similarity ( from_vector , to_vector ) if to_list is None : np . fill_diagonal ( similarity_matrix , 0 ) indices = np . flip ( np . argsort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = np . flip ( np . sort ( similarity_matrix , axis =- 1 ), axis = 1 )[:, : top_n ] similarities = [ np . round ( similarities [:, i ], 3 ) for i in range ( similarities . shape [ 1 ])] # Convert results to df if to_list is None : to_list = from_list . copy () columns = ([ \"From\" ] + [ \"To\" if i == 0 else f \"To_ { i + 1 } \" for i in range ( top_n )] + [ \"Similarity\" if i == 0 else f \"Similarity_ { i + 1 } \" for i in range ( top_n )]) matches = [[ to_list [ idx ] for idx in indices [:, i ]] for i in range ( indices . shape [ 1 ])] matches = pd . DataFrame ( np . vstack (([ from_list ], matches , similarities )) . T , columns = columns ) # Update column order columns = [[ \"From\" , \"To\" , \"Similarity\" ]] + [[ f \"To_ { i + 2 } \" , f \"Similarity_ { i + 2 } \" ] for i in range (( top_n - 1 ))] matches = matches . loc [:, [ title for column in columns for title in column ]] # Update types for column in matches . columns : if \"Similarity\" in column : matches [ column ] = matches [ column ] . astype ( float ) matches . loc [ matches [ column ] < 0.001 , column ] = float ( 0 ) matches . loc [ matches [ column ] < 0.001 , column . replace ( \"Similarity\" , \"To\" )] = None return matches","title":"polyfuzz.models.cosine_similarity"},{"location":"api/models/rapidfuzz/","text":"polyfuzz.models.RapidFuzz \u00b6 Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters: Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 score_cutoff float The minimum similarity for which to return a good match. Should be between 0 and 1. 0 scorer Callable The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. <cyfunction WRatio at 0x0000020BAAB935F0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) Source code in polyfuzz\\models\\_rapidfuzz.py class RapidFuzz ( BaseMatcher ): \"\"\" Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Arguments: n_jobs: Nr of parallel processes, use -1 to use all cores score_cutoff: The minimum similarity for which to return a good match. Should be between 0 and 1. scorer: The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. model_id: The name of the particular instance, used when comparing models Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) ``` \"\"\" def __init__ ( self , n_jobs : int = 1 , score_cutoff : float = 0 , scorer : Callable = fuzz . WRatio , model_id : str = None ): super () . __init__ ( model_id ) self . type = \"EditDistance\" self . score_cutoff = score_cutoff * 100 self . scorer = scorer self . equal_lists = False if n_jobs == - 1 : self . n_jobs = cpu_count () else : self . n_jobs = n_jobs def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches def _calculate_edit_distance ( self , from_string : str , to_list : List [ str ]) -> Tuple [ str , Union [ str , None ], float ]: \"\"\" Calculate the edit distance between a string and a list \"\"\" if self . equal_lists : to_list . remove ( from_string ) match = process . extractOne ( from_string , to_list , score_cutoff = self . score_cutoff , scorer = self . scorer ) if match : return from_string , match [ 0 ], match [ 1 ] / 100 else : return from_string , None , 0. match ( self , from_list , to_list = None , ** kwargs ) \u00b6 Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns: Type Description matches The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_rapidfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches","title":"RapidFuzz"},{"location":"api/models/rapidfuzz/#polyfuzzmodelsrapidfuzz","text":"Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Parameters: Name Type Description Default n_jobs int Nr of parallel processes, use -1 to use all cores 1 score_cutoff float The minimum similarity for which to return a good match. Should be between 0 and 1. 0 scorer Callable The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. <cyfunction WRatio at 0x0000020BAAB935F0> model_id str The name of the particular instance, used when comparing models None Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) Source code in polyfuzz\\models\\_rapidfuzz.py class RapidFuzz ( BaseMatcher ): \"\"\" Calculate the Edit Distance between lists of strings using RapidFuzz's process function We are using RapidFuzz instead of FuzzyWuzzy since it is much faster and does not require the more restrictive GPL license Arguments: n_jobs: Nr of parallel processes, use -1 to use all cores score_cutoff: The minimum similarity for which to return a good match. Should be between 0 and 1. scorer: The scorer function to be used to calculate the edit distance Options: * fuzz.ratio * fuzz.partial_ratio * fuzz.token_sort_ratio * fuzz.partial_token_sort_ratio * fuzz.token_set_ratio * fuzz.partial_token_set_ratio * fuzz.token_ratio * fuzz.partial_token_ratio * fuzz.WRation * fuzz.QRatio See https://maxbachmann.github.io/rapidfuzz/usage/fuzz/ for an extensive description of the scoring methods. model_id: The name of the particular instance, used when comparing models Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) ``` \"\"\" def __init__ ( self , n_jobs : int = 1 , score_cutoff : float = 0 , scorer : Callable = fuzz . WRatio , model_id : str = None ): super () . __init__ ( model_id ) self . type = \"EditDistance\" self . score_cutoff = score_cutoff * 100 self . scorer = scorer self . equal_lists = False if n_jobs == - 1 : self . n_jobs = cpu_count () else : self . n_jobs = n_jobs def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches def _calculate_edit_distance ( self , from_string : str , to_list : List [ str ]) -> Tuple [ str , Union [ str , None ], float ]: \"\"\" Calculate the edit distance between a string and a list \"\"\" if self . equal_lists : to_list . remove ( from_string ) match = process . extractOne ( from_string , to_list , score_cutoff = self . score_cutoff , scorer = self . scorer ) if match : return from_string , match [ 0 ], match [ 1 ] / 100 else : return from_string , None , 0.","title":"polyfuzz.models.RapidFuzz"},{"location":"api/models/rapidfuzz/#polyfuzz.models._rapidfuzz.RapidFuzz.match","text":"Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None Returns: Type Description matches The best matches between the lists of strings Usage: from rapidfuzz import fuzz model = RapidFuzz ( n_jobs =- 1 , score_cutoff = 0.5 , scorer = fuzz . WRatio ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_rapidfuzz.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , ** kwargs ) -> pd . DataFrame : \"\"\" Calculate the edit distances between two list of strings by parallelizing the calculation and passing the lists in batches. Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to Returns: matches: The best matches between the lists of strings Usage: ```python from rapidfuzz import fuzz model = RapidFuzz(n_jobs=-1, score_cutoff=0.5, scorer=fuzz.WRatio) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" if to_list is None : self . equal_lists = True expected_iterations = int ( len ( from_list ) / 2 ) to_list = from_list . copy () else : expected_iterations = len ( from_list ) matches = Parallel ( n_jobs = self . n_jobs )( delayed ( self . _calculate_edit_distance ) ( from_string , to_list ) for from_string in tqdm ( from_list , total = expected_iterations , disable = True )) matches = pd . DataFrame ( matches , columns = [ 'From' , \"To\" , \"Similarity\" ]) return matches","title":"match()"},{"location":"api/models/sbert/","text":"polyfuzz.models.SentenceEmbeddings \u00b6 Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters: Name Type Description Default embedding_model Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer] The sbert model to use, this can be either a string or the model directly 'all-MiniLM-L6-v2' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" , min_similarity = 0.5 ) Or if you want to directly pass a sbert model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model , min_similarity = 0.5 ) Source code in polyfuzz\\models\\_sbert.py class SentenceEmbeddings ( BaseMatcher ): \"\"\" Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Arguments: embedding_model: The sbert model to use, this can be either a string or the model directly min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned cosine_method: The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: ```python distance_model = SentenceEmbeddings(\"all-MiniLM-L6-v2\", min_similarity=0.5) ``` Or if you want to directly pass a sbert model: ```python from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") distance_model = SentenceEmbeddings(embedding_model, min_similarity=0.5) ``` \"\"\" def __init__ ( self , embedding_model : Union [ str , SentenceTransformer ] = \"all-MiniLM-L6-v2\" , min_similarity : float = 0.75 , top_n : int = 1 , cosine_method : str = \"sparse\" , model_id : str = None ): super () . __init__ ( model_id ) self . type = \"Embeddings\" if isinstance ( embedding_model , SentenceTransformer ): self . embedding_model = embedding_model elif isinstance ( embedding_model , str ): self . embedding_model = SentenceTransformer ( embedding_model ) else : raise ValueError ( \"Please select a correct SentenceTransformers model: \\n \" \"`from sentence_transformers import SentenceTransformer` \\n \" \"`embedding_model = SentenceTransformer('all-MiniLM-L6-v2')`\" ) self . min_similarity = min_similarity self . top_n = top_n self . cosine_method = cosine_method self . embeddings_to = None def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches match ( self , from_list , to_list = None , embeddings_from = None , embeddings_to = None , re_train = True ) \u00b6 Matches the two lists of strings to each other and returns the best mapping Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns: Type Description matches The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_sbert.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches","title":"SBERT"},{"location":"api/models/sbert/#polyfuzzmodelssentenceembeddings","text":"Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Parameters: Name Type Description Default embedding_model Union[str, sentence_transformers.SentenceTransformer.SentenceTransformer] The sbert model to use, this can be either a string or the model directly 'all-MiniLM-L6-v2' min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of best matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" , min_similarity = 0.5 ) Or if you want to directly pass a sbert model: from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model , min_similarity = 0.5 ) Source code in polyfuzz\\models\\_sbert.py class SentenceEmbeddings ( BaseMatcher ): \"\"\" Embed words into vectors and use cosine similarity to find the best matches between two lists of strings Arguments: embedding_model: The sbert model to use, this can be either a string or the model directly min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of best matches you want returned cosine_method: The method/package for calculating the cosine similarity. Options: \"sparse\", \"sklearn\", \"knn\". Sparse is the fastest and most memory efficient but requires a package that might be difficult to install. Sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse Knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: ```python distance_model = SentenceEmbeddings(\"all-MiniLM-L6-v2\", min_similarity=0.5) ``` Or if you want to directly pass a sbert model: ```python from sentence_transformers import SentenceTransformer embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") distance_model = SentenceEmbeddings(embedding_model, min_similarity=0.5) ``` \"\"\" def __init__ ( self , embedding_model : Union [ str , SentenceTransformer ] = \"all-MiniLM-L6-v2\" , min_similarity : float = 0.75 , top_n : int = 1 , cosine_method : str = \"sparse\" , model_id : str = None ): super () . __init__ ( model_id ) self . type = \"Embeddings\" if isinstance ( embedding_model , SentenceTransformer ): self . embedding_model = embedding_model elif isinstance ( embedding_model , str ): self . embedding_model = SentenceTransformer ( embedding_model ) else : raise ValueError ( \"Please select a correct SentenceTransformers model: \\n \" \"`from sentence_transformers import SentenceTransformer` \\n \" \"`embedding_model = SentenceTransformer('all-MiniLM-L6-v2')`\" ) self . min_similarity = min_similarity self . top_n = top_n self . cosine_method = cosine_method self . embeddings_to = None def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches","title":"polyfuzz.models.SentenceEmbeddings"},{"location":"api/models/sbert/#polyfuzz.models._sbert.SentenceEmbeddings.match","text":"Matches the two lists of strings to each other and returns the best mapping Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None embeddings_from ndarray Embeddings you created yourself from the from_list None embeddings_to ndarray Embeddings you created yourself from the to_list None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns: Type Description matches The best matches between the lists of strings Usage: model = Embeddings ( min_similarity = 0.5 ) matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_sbert.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , embeddings_from : np . ndarray = None , embeddings_to : np . ndarray = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Matches the two lists of strings to each other and returns the best mapping Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to embeddings_from: Embeddings you created yourself from the `from_list` embeddings_to: Embeddings you created yourself from the `to_list` re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python model = Embeddings(min_similarity=0.5) matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) self . embeddings_to = embeddings_to return matches","title":"match()"},{"location":"api/models/spacy/","text":"polyfuzz.models.SpacyEmbeddings \u00b6","title":"Spacy"},{"location":"api/models/spacy/#polyfuzzmodelsspacyembeddings","text":"","title":"polyfuzz.models.SpacyEmbeddings"},{"location":"api/models/tfidf/","text":"polyfuzz.models.TFIDF \u00b6 A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters: Name Type Description Default n_gram_range Tuple[int, int] The n_gram_range on a character-level (3, 3) clean_string bool Whether to clean the string such that only alphanumerical characters are kept True min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: from polymatcher.models import TFIDF model = TFIDF ( n_gram_range = ( 3 , 3 ), clean_string = True , use_knn = False ) Source code in polyfuzz\\models\\_tfidf.py class TFIDF ( BaseMatcher ): \"\"\" A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Arguments: n_gram_range: The n_gram_range on a character-level clean_string: Whether to clean the string such that only alphanumerical characters are kept min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of matches you want returned cosine_method: The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: ```python from polymatcher.models import TFIDF model = TFIDF(n_gram_range=(3, 3), clean_string=True, use_knn=False) ``` \"\"\" def __init__ ( self , n_gram_range : Tuple [ int , int ] = ( 3 , 3 ), clean_string : bool = True , min_similarity : float = 0.75 , top_n : int = 1 , cosine_method : str = \"sparse\" , model_id : str = None ): super () . __init__ ( model_id ) self . type = \"TF-IDF\" self . n_gram_range = n_gram_range self . clean_string = clean_string self . min_similarity = min_similarity self . cosine_method = cosine_method self . top_n = top_n self . vectorizer = None self . tf_idf_to = None def match ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list , re_train ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches def _extract_tf_idf ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Calculate distances between TF-IDF vectors of from_list and to_list \"\"\" if to_list : if re_train : self . vectorizer = TfidfVectorizer ( min_df = 1 , analyzer = self . _create_ngrams ) . fit ( to_list + from_list ) self . tf_idf_to = self . vectorizer . transform ( to_list ) tf_idf_from = self . vectorizer . transform ( from_list ) else : if re_train : self . vectorizer = TfidfVectorizer ( min_df = 1 , analyzer = self . _create_ngrams ) . fit ( from_list ) self . tf_idf_to = self . vectorizer . transform ( from_list ) tf_idf_from = self . tf_idf_to return tf_idf_from , self . tf_idf_to def _create_ngrams ( self , string : str ) -> List [ str ]: \"\"\" Create n_grams from a string Steps: * Extract character-level ngrams with `self.n_gram_range` (both ends inclusive) * Remove n-grams that have a whitespace in them \"\"\" if self . clean_string : string = _clean_string ( string ) result = [] for n in range ( self . n_gram_range [ 0 ], self . n_gram_range [ 1 ] + 1 ): ngrams = zip ( * [ string [ i :] for i in range ( n )]) ngrams = [ '' . join ( ngram ) for ngram in ngrams if ' ' not in ngram ] result . extend ( ngrams ) return result match ( self , from_list , to_list = None , re_train = True ) \u00b6 Match two lists of strings to each other and return the most similar strings Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns: Type Description matches The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF () matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_tfidf.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list , re_train ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches","title":"TFIDF"},{"location":"api/models/tfidf/#polyfuzzmodelstfidf","text":"A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Parameters: Name Type Description Default n_gram_range Tuple[int, int] The n_gram_range on a character-level (3, 3) clean_string bool Whether to clean the string such that only alphanumerical characters are kept True min_similarity float The minimum similarity between strings, otherwise return 0 similarity 0.75 top_n int The number of matches you want returned 1 cosine_method str The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory 'sparse' model_id str The name of the particular instance, used when comparing models None Usage: from polymatcher.models import TFIDF model = TFIDF ( n_gram_range = ( 3 , 3 ), clean_string = True , use_knn = False ) Source code in polyfuzz\\models\\_tfidf.py class TFIDF ( BaseMatcher ): \"\"\" A character based n-gram TF-IDF to approximate edit distance We turn a string into, typically of length 3, n-grams. For example, using 3-grams of the \"hotel\" we get ['hot', 'ote', 'tel']. These are then used as input for a TfidfVectorizer in order to create a vector for each word. Then, we simply apply cosine similarity through k-NN Arguments: n_gram_range: The n_gram_range on a character-level clean_string: Whether to clean the string such that only alphanumerical characters are kept min_similarity: The minimum similarity between strings, otherwise return 0 similarity top_n: The number of matches you want returned cosine_method: The method/package for calculating the cosine similarity. Options: * sparse * sklearn * knn sparse is the fastest and most memory efficient but requires a package that might be difficult to install sklearn is a bit slower than sparse and requires significantly more memory as the distance matrix is not sparse knn uses 1-nearest neighbor to extract the most similar strings it is significantly slower than both methods but requires little memory model_id: The name of the particular instance, used when comparing models Usage: ```python from polymatcher.models import TFIDF model = TFIDF(n_gram_range=(3, 3), clean_string=True, use_knn=False) ``` \"\"\" def __init__ ( self , n_gram_range : Tuple [ int , int ] = ( 3 , 3 ), clean_string : bool = True , min_similarity : float = 0.75 , top_n : int = 1 , cosine_method : str = \"sparse\" , model_id : str = None ): super () . __init__ ( model_id ) self . type = \"TF-IDF\" self . n_gram_range = n_gram_range self . clean_string = clean_string self . min_similarity = min_similarity self . cosine_method = cosine_method self . top_n = top_n self . vectorizer = None self . tf_idf_to = None def match ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list , re_train ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches def _extract_tf_idf ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> Tuple [ np . ndarray , np . ndarray ]: \"\"\" Calculate distances between TF-IDF vectors of from_list and to_list \"\"\" if to_list : if re_train : self . vectorizer = TfidfVectorizer ( min_df = 1 , analyzer = self . _create_ngrams ) . fit ( to_list + from_list ) self . tf_idf_to = self . vectorizer . transform ( to_list ) tf_idf_from = self . vectorizer . transform ( from_list ) else : if re_train : self . vectorizer = TfidfVectorizer ( min_df = 1 , analyzer = self . _create_ngrams ) . fit ( from_list ) self . tf_idf_to = self . vectorizer . transform ( from_list ) tf_idf_from = self . tf_idf_to return tf_idf_from , self . tf_idf_to def _create_ngrams ( self , string : str ) -> List [ str ]: \"\"\" Create n_grams from a string Steps: * Extract character-level ngrams with `self.n_gram_range` (both ends inclusive) * Remove n-grams that have a whitespace in them \"\"\" if self . clean_string : string = _clean_string ( string ) result = [] for n in range ( self . n_gram_range [ 0 ], self . n_gram_range [ 1 ] + 1 ): ngrams = zip ( * [ string [ i :] for i in range ( n )]) ngrams = [ '' . join ( ngram ) for ngram in ngrams if ' ' not in ngram ] result . extend ( ngrams ) return result","title":"polyfuzz.models.TFIDF"},{"location":"api/models/tfidf/#polyfuzz.models._tfidf.TFIDF.match","text":"Match two lists of strings to each other and return the most similar strings Parameters: Name Type Description Default from_list List[str] The list from which you want mappings required to_list List[str] The list where you want to map to None re_train bool Whether to re-train the model with new embeddings Set this to False if you want to use this model in production True Returns: Type Description matches The best matches between the lists of strings Usage: from polymatcher.models import TFIDF model = TFIDF () matches = model . match ([ \"string_one\" , \"string_two\" ], [ \"string_three\" , \"string_four\" ]) Source code in polyfuzz\\models\\_tfidf.py def match ( self , from_list : List [ str ], to_list : List [ str ] = None , re_train : bool = True ) -> pd . DataFrame : \"\"\" Match two lists of strings to each other and return the most similar strings Arguments: from_list: The list from which you want mappings to_list: The list where you want to map to re_train: Whether to re-train the model with new embeddings Set this to False if you want to use this model in production Returns: matches: The best matches between the lists of strings Usage: ```python from polymatcher.models import TFIDF model = TFIDF() matches = model.match([\"string_one\", \"string_two\"], [\"string_three\", \"string_four\"]) ``` \"\"\" tf_idf_from , tf_idf_to = self . _extract_tf_idf ( from_list , to_list , re_train ) matches = cosine_similarity ( tf_idf_from , tf_idf_to , from_list , to_list , self . min_similarity , top_n = self . top_n , method = self . cosine_method ) return matches","title":"match()"},{"location":"api/models/use/","text":"polyfuzz.models.USEEmbeddings \u00b6","title":"USE"},{"location":"api/models/use/#polyfuzzmodelsuseembeddings","text":"","title":"polyfuzz.models.USEEmbeddings"},{"location":"tutorial/basematcher/basematcher/","text":"Custom Models \u00b6 Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel ( BaseMatcher ): def match ( self , from_list , to_list , ** kwargs ): # Calculate distances matches = [[ fuzz . ratio ( from_string , to_string ) / 100 for to_string in to_list ] for from_string in from_list ] # Get best matches mappings = [ to_list [ index ] for index in np . argmax ( matches , axis = 1 )] scores = np . max ( matches , axis = 1 ) # Prepare dataframe matches = pd . DataFrame ({ 'From' : from_list , 'To' : mappings , 'Similarity' : scores }) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . match ( from_list , to_list ) Now we can visualize the results: model . visualize_precision_recall ( kde = True ) fit, transform, fit_transform \u00b6 Although the above model can be used in production using fit , it does not track its state between fit and transform . This is not necessary here, since edit distances should be recalculated but if you have embeddings that you do not want to re-calculate, then it is helpful to track the states between fit and transform so that embeddings do not need to be re-calculated. To do so, we can use the re_train parameter to define what happens if we re-train a model (for example when using fit ) and what happens when we do not re-train a model (for example when using transform ). In the example below, when we set re_train=True we calculate the embeddings from both the from_list and to_list if they are defined and save the embeddings to the self.embeddings_to variable. Then, when we set re_train=True , we can prevent redoing the fit by leveraging the pre-calculated self.embeddings_to variable. import numpy as np from sentence_transformers import SentenceTransformer from ._utils import cosine_similarity from ._base import BaseMatcher class SentenceEmbeddings ( BaseMatcher ): def __init__ ( self , model_id ): super () . __init__ ( model_id ) self . type = \"Embeddings\" self . embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) self . embeddings_to = None def match ( self , from_list , to_list , re_train = True ) -> pd . DataFrame : # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) # Extract matches matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list ) self . embeddings_to = embeddings_to return matches Then, we can use it as follows: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . fit ( from_list ) By using the .fit function, embeddings are created from the from_list variable and saved. Then, when we run model.transform(to_list) , the embeddings created from the from_list variable do not need to be recalculated.","title":"Custom Models"},{"location":"tutorial/basematcher/basematcher/#custom-models","text":"Although PolyFuzz has several models implemented, what if you have developed your own? What if you want a different similarity/distance measure that is not defined in PolyFuzz? That is where custom models come in. If you follow the structure of PolyFuzz's BaseMatcher you can quickly implement any model you would like. You simply create a class using BaseMatcher , make sure it has a function match that inputs two lists and outputs a pandas dataframe. That's it! We start by creating our own model that implements the ratio similarity measure from RapidFuzz: import numpy as np import pandas as pd from rapidfuzz import fuzz from polyfuzz import PolyFuzz from polyfuzz.models import BaseMatcher class MyModel ( BaseMatcher ): def match ( self , from_list , to_list , ** kwargs ): # Calculate distances matches = [[ fuzz . ratio ( from_string , to_string ) / 100 for to_string in to_list ] for from_string in from_list ] # Get best matches mappings = [ to_list [ index ] for index in np . argmax ( matches , axis = 1 )] scores = np . max ( matches , axis = 1 ) # Prepare dataframe matches = pd . DataFrame ({ 'From' : from_list , 'To' : mappings , 'Similarity' : scores }) return matches MyModel can now be used within PolyFuzz and runs like every other model: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . match ( from_list , to_list ) Now we can visualize the results: model . visualize_precision_recall ( kde = True )","title":"Custom Models"},{"location":"tutorial/basematcher/basematcher/#fit-transform-fit_transform","text":"Although the above model can be used in production using fit , it does not track its state between fit and transform . This is not necessary here, since edit distances should be recalculated but if you have embeddings that you do not want to re-calculate, then it is helpful to track the states between fit and transform so that embeddings do not need to be re-calculated. To do so, we can use the re_train parameter to define what happens if we re-train a model (for example when using fit ) and what happens when we do not re-train a model (for example when using transform ). In the example below, when we set re_train=True we calculate the embeddings from both the from_list and to_list if they are defined and save the embeddings to the self.embeddings_to variable. Then, when we set re_train=True , we can prevent redoing the fit by leveraging the pre-calculated self.embeddings_to variable. import numpy as np from sentence_transformers import SentenceTransformer from ._utils import cosine_similarity from ._base import BaseMatcher class SentenceEmbeddings ( BaseMatcher ): def __init__ ( self , model_id ): super () . __init__ ( model_id ) self . type = \"Embeddings\" self . embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) self . embeddings_to = None def match ( self , from_list , to_list , re_train = True ) -> pd . DataFrame : # Extract embeddings from the `from_list` embeddings_from = self . embedding_model . encode ( from_list , show_progress_bar = False ) # Extract embeddings from the `to_list` if it exists if not isinstance ( embeddings_to , np . ndarray ): if not re_train : embeddings_to = self . embeddings_to elif to_list is None : embeddings_to = self . embedding_model . encode ( from_list , show_progress_bar = False ) else : embeddings_to = self . embedding_model . encode ( to_list , show_progress_bar = False ) # Extract matches matches = cosine_similarity ( embeddings_from , embeddings_to , from_list , to_list ) self . embeddings_to = embeddings_to return matches Then, we can use it as follows: from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] custom_matcher = MyModel () model = PolyFuzz ( custom_matcher ) . fit ( from_list ) By using the .fit function, embeddings are created from the from_list variable and saved. Then, when we run model.transform(to_list) , the embeddings created from the from_list variable do not need to be recalculated.","title":"fit, transform, fit_transform"},{"location":"tutorial/datasets/datasets/","text":"Datasets \u00b6 There are two datasets prepared for you to play around with: Company Names Movie Titles Movie Titles \u00b6 This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles () model = PolyFuzz ( \"TF-IDF\" ) . match ( data [ \"Netflix\" ], data [ \"IMDB\" ]) Company Names \u00b6 This data is retrieved from here and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names () model = PolyFuzz ( \"TF-IDF\" ) . match ( data ) By only inserting a single list, PolyFuzz will recognize that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Datasets"},{"location":"tutorial/datasets/datasets/#datasets","text":"There are two datasets prepared for you to play around with: Company Names Movie Titles","title":"Datasets"},{"location":"tutorial/datasets/datasets/#movie-titles","text":"This data is retrieved from: https://www.kaggle.com/stefanoleone992/imdb-extensive-dataset https://www.kaggle.com/shivamb/netflix-shows It contains Netflix and IMDB movie titles that can be matched against each other. Where IMDB has 80852 movie titles and Netflix has 6172 movie titles. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_movie_titles data = load_movie_titles () model = PolyFuzz ( \"TF-IDF\" ) . match ( data [ \"Netflix\" ], data [ \"IMDB\" ])","title":"Movie Titles"},{"location":"tutorial/datasets/datasets/#company-names","text":"This data is retrieved from here and contains 100_000 company names to be matched against each other. This is a different use case than what you have typically seen so far. We often see two different lists compared with each other. Here, you can use this dataset to compare the company names with themselves in order to clean them up. You can use them as follows: from polyfuzz import PolyFuzz from polyfuzz.datasets import load_company_names data = load_company_names () model = PolyFuzz ( \"TF-IDF\" ) . match ( data ) By only inserting a single list, PolyFuzz will recognize that you are looking to match the titles with themselves. It will ignore any comparison a string has with itself, otherwise everything will get mapped to itself.","title":"Company Names"},{"location":"tutorial/grouper/grouper/","text":"Custom Grouper \u00b6 The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) # Custom grouper base_edit_grouper = EditDistance ( n_jobs = 1 ) model . group ( base_edit_grouper ) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/grouper/grouper/#custom-grouper","text":"The basic grouper is a TF-IDF implementation that uses single linkage to group the strings you mapped to together. With the customizability philosophy of PolyFuzz in mind it is not unexpected that you can also use any of the models, and even custom models, as your grouper! Here, we use Edit Distance instead of TF-IDF to group the strings we mapped to: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) # Custom grouper base_edit_grouper = EditDistance ( n_jobs = 1 ) model . group ( base_edit_grouper ) And that is it! We have now grouped our matches we mapped to together using Edit Distance instead of TF-IDF.","title":"Custom Grouper"},{"location":"tutorial/models/models/","text":"Models \u00b6 Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers 5. SentenceTransformers 6. Gensim 7. Spacy With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers. TF-IDF \u00b6 Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF\" ) model = PolyFuzz ( tfidf ) . match ( from_list , to_list ) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all. EditDistance \u00b6 There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] jellyfish_matcher = EditDistance ( n_jobs = 1 , scorer = jaro_winkler_similarity ) model = PolyFuzz ( jellyfish_matcher ) . match ( from_list , to_list ) RapidFuzz \u00b6 Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] rapidfuzz_matcher = RapidFuzz ( n_jobs = 1 ) model = PolyFuzz ( rapidfuzz_matcher ) . match ( from_list , to_list ) Embeddings \u00b6 With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) models = PolyFuzz ( bert_matcher ) . match ( from_list , to_list ) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings , WordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) fasttext = WordEmbeddings ( 'en-crawl' ) fasttext_matcher = Embeddings ( fasttext , min_similarity = 0 ) matchers = [ bert_matcher , fasttext_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) SentenceTransformers \u00b6 We can use sentence-transformers to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SentenceEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom SentenceTransformer model: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from sentence_transformers import SentenceTransformer from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Gensim \u00b6 We can use gensim to load in a word embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our GensimEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = GensimEmbeddings ( \"glove-twitter-25\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Gensim model: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings import gensim.downloader as api from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = api . load ( \"glove-twitter-25\" ) distance_model = GensimEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Spacy \u00b6 We can use spacy to load in an embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SpacyEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SpacyEmbeddings ( \"en_core_web_md\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Spacy model: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings import spacy from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) distance_model = SpacyEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Universal Sentence Encoder (USE) \u00b6 The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the strings. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. We simply have to instantiate our USEEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom USE model: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings import tensorflow_hub from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) distance_model = USEEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) Using Multiple Models \u00b6 from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance , TFIDF , Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 , model_id = \"BERT\" ) tfidf_matcher = TFIDF ( min_similarity = 0 ) edit_matcher = EditDistance () matchers = [ bert_matcher , tfidf_matcher , edit_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models . get_matches ( \"BERT\" ) From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models . visualize_precision_recall ( kde = True )","title":"Models"},{"location":"tutorial/models/models/#models","text":"Currently, the following models are implemented in PolyFuzz: 1. TF-IDF 2. EditDistance with RapidFuzz 3. FastText and GloVe 4. \ud83e\udd17 Transformers 5. SentenceTransformers 6. Gensim 7. Spacy With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz. All models listed above can be found in polyfuzz.models and can be used to create and compare different matchers.","title":"Models"},{"location":"tutorial/models/models/#tf-idf","text":"Although the terms in TF-IDF are usually words, we are going to be using TF-IDF on a character-level. We will be extracting n-grams from a string and count the frequency of these n-grams across all input strings. For example, with 3-grams, the word \"hotel\" can be defined as \"hot\", \"ote\", and \"tel\". After generating the n-grams and applying TF-IDF on these \"terms\", we can use cosine similarity to compare the generated TF-IDF vectors against each other. We simply load in TFIDF from polyfuzz.models and pass it to our PolyFuzz instance: from polyfuzz.models import TFIDF from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] tfidf = TFIDF ( n_gram_range = ( 3 , 3 ), min_similarity = 0 , model_id = \"TF-IDF\" ) model = PolyFuzz ( tfidf ) . match ( from_list , to_list ) and that's it! You can play around with the TFIDF matcher until you get the results you are looking for. Note that if you increase the min_similarity there is a chance that some strings will not be matched at all.","title":"TF-IDF"},{"location":"tutorial/models/models/#editdistance","text":"There are many edit distance functions one could use and the EditDistance model from polyfuzz.models allows you to pass in any distance function. As long as that distance function takes in two strings and spits out a float, you can pass anything! In the example below, we are going to be using Jaro Winkler Similarity from the jellyfish package to create our custom scorer: from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance from jellyfish import jaro_winkler_similarity from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] jellyfish_matcher = EditDistance ( n_jobs = 1 , scorer = jaro_winkler_similarity ) model = PolyFuzz ( jellyfish_matcher ) . match ( from_list , to_list )","title":"EditDistance"},{"location":"tutorial/models/models/#rapidfuzz","text":"Edit distance measures are typically quite slow. Moreover, the one that is heavily used, fuzzywuzzy , has a very restrictive licence (GPL). Instead, I decided to create a RapidFuzz matcher which is a fast implementation of fuzzywuzzy and has a less restrictive licence (MIT): from polyfuzz import PolyFuzz from polyfuzz.models import RapidFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] rapidfuzz_matcher = RapidFuzz ( n_jobs = 1 ) model = PolyFuzz ( rapidfuzz_matcher ) . match ( from_list , to_list )","title":"RapidFuzz"},{"location":"tutorial/models/models/#embeddings","text":"With Flair , we can use all \ud83e\udd17 Transformers that are publicly available . The embeddings that are created are compared with cosine similarity in order to understand how similar the created embeddings are to each other. We simply have to instantiate any Flair WordEmbedding method and pass it through PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) models = PolyFuzz ( bert_matcher ) . match ( from_list , to_list ) Flair allows you to use pool word embeddings to create more powerful word embeddings. Below, we pool FastText and BERT to create a single embedding representation from which we can calculate the similarity between strings: from polyfuzz import PolyFuzz from polyfuzz.models import Embeddings from flair.embeddings import TransformerWordEmbeddings , WordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 ) fasttext = WordEmbeddings ( 'en-crawl' ) fasttext_matcher = Embeddings ( fasttext , min_similarity = 0 ) matchers = [ bert_matcher , fasttext_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list )","title":"Embeddings"},{"location":"tutorial/models/models/#sentencetransformers","text":"We can use sentence-transformers to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SentenceEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SentenceEmbeddings ( \"all-MiniLM-L6-v2\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom SentenceTransformer model: from polyfuzz import PolyFuzz from polyfuzz.models import SentenceEmbeddings from sentence_transformers import SentenceTransformer from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = SentenceTransformer ( \"all-MiniLM-L6-v2\" ) distance_model = SentenceEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"SentenceTransformers"},{"location":"tutorial/models/models/#gensim","text":"We can use gensim to load in a word embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our GensimEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = GensimEmbeddings ( \"glove-twitter-25\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Gensim model: from polyfuzz import PolyFuzz from polyfuzz.models import GensimEmbeddings import gensim.downloader as api from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = api . load ( \"glove-twitter-25\" ) distance_model = GensimEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"Gensim"},{"location":"tutorial/models/models/#spacy","text":"We can use spacy to load in an embedding model to generate embeddings from our input list and find the closest matching entities using cosine similarity. We simply have to instantiate our SpacyEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = SpacyEmbeddings ( \"en_core_web_md\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom Spacy model: from polyfuzz import PolyFuzz from polyfuzz.models import SpacyEmbeddings import spacy from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = spacy . load ( \"en_core_web_md\" , exclude = [ 'tagger' , 'parser' , 'ner' , 'attribute_ruler' , 'lemmatizer' ]) distance_model = SpacyEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"Spacy"},{"location":"tutorial/models/models/#universal-sentence-encoder-use","text":"The Universal Sentence Encoder encodes text into high-dimensional vectors that are used here for embedding the strings. The model is trained and optimized for greater-than-word length text, such as sentences, phrases, or short paragraphs. We simply have to instantiate our USEEmbeddings class and pass it to PolyFuzz: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] distance_model = USEEmbeddings ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list ) For a full list of possible models, click this link. You can also use a custom USE model: from polyfuzz import PolyFuzz from polyfuzz.models import USEEmbeddings import tensorflow_hub from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] embedding_model = tensorflow_hub . load ( \"https://tfhub.dev/google/universal-sentence-encoder/4\" ) distance_model = USEEmbeddings ( embedding_model ) model = PolyFuzz ( distance_model ) . match ( from_list , to_list )","title":"Universal Sentence Encoder (USE)"},{"location":"tutorial/models/models/#using-multiple-models","text":"from polyfuzz import PolyFuzz from polyfuzz.models import EditDistance , TFIDF , Embeddings from flair.embeddings import TransformerWordEmbeddings from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] bert = TransformerWordEmbeddings ( 'bert-base-multilingual-cased' ) bert_matcher = Embeddings ( bert , min_similarity = 0 , model_id = \"BERT\" ) tfidf_matcher = TFIDF ( min_similarity = 0 ) edit_matcher = EditDistance () matchers = [ bert_matcher , tfidf_matcher , edit_matcher ] models = PolyFuzz ( matchers ) . match ( from_list , to_list ) To access the results, we again can call get_matches but since we have multiple models we get back a dictionary of dataframes back. In order to access the results of a specific model, call get_matches with the correct id: >>> models . get_matches ( \"BERT\" ) From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.928045 3 recal apples 0.825268 4 house mouse 0.887524 5 similarity mouse 0.791548 Finally, visualize the results to compare the models: models . visualize_precision_recall ( kde = True )","title":"Using Multiple Models"},{"location":"tutorial/quickstart/quickstart/","text":"Installation \u00b6 You can install PolyFuzz via pip: pip install polyfuzz You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install polyfuzz [ sbert ] pip install polyfuzz [ flair ] pip install polyfuzz [ gensim ] pip install polyfuzz [ spacy ] pip install polyfuzz [ use ] If you want to speed up the cosine similarity comparison and decrease memory usage when using embedding models, you can use sparse_dot_topn which is installed via: pip install polyfuzz [ fast ] Getting Started \u00b6 The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) The resulting matches can be accessed through model.get_matches() : >>> model . get_matches () From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively. Fit / Transform \u00b6 The .match function allows you to quickly extract similar strings. However, after selecting the right models to be used, you may want to use PolyFuzz in production to match incoming strings. To do so, we can make use of the familiar fit , transform , and fit_transform functions. Let's say that we have a list of words that we know to be correct called train_words . We want to any incoming word to mapped to one of the words in train_words . In other words, we fit on train_words and we use transform on any incoming words: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] unseen_words = [ \"apple\" , \"apples\" , \"mouse\" ] # Fit model = PolyFuzz ( \"TF-IDF\" ) model . fit ( train_words ) # Transform results = model . transform ( unseen_words ) In the above example, we are using fit on train_words to calculate the TF-IDF representation of those words which are saved to be used again in transform . This speeds up transform quite a bit since all TF-IDF representations are stored when applying fit . Save / Load \u00b6 We can save and load the model as follows to be used in production: # Save the model model . save ( \"my_model\" ) # Load the model loaded_model = PolyFuzz . load ( \"my_model\" ) Group Matches \u00b6 We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model . group ( link_min_similarity = 0.75 ) >>> model . get_matches () From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples . Precision-Recall Curve \u00b6 Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Quickstart"},{"location":"tutorial/quickstart/quickstart/#installation","text":"You can install PolyFuzz via pip: pip install polyfuzz You may want to install more depending on the transformers and language backends that you will be using. The possible installations are: pip install polyfuzz [ sbert ] pip install polyfuzz [ flair ] pip install polyfuzz [ gensim ] pip install polyfuzz [ spacy ] pip install polyfuzz [ use ] If you want to speed up the cosine similarity comparison and decrease memory usage when using embedding models, you can use sparse_dot_topn which is installed via: pip install polyfuzz [ fast ]","title":"Installation"},{"location":"tutorial/quickstart/quickstart/#getting-started","text":"The main goal of PolyFuzz is to allow the user to perform different methods for matching strings. We start by defining two lists, one to map from and one to map to. We are going to be using TF-IDF to create n-grams on a character level in order to compare similarity between strings. We only have to instantiate PolyFuzz with TF-IDF and match the lists: from polyfuzz import PolyFuzz from_list = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] to_list = [ \"apple\" , \"apples\" , \"mouse\" ] model = PolyFuzz ( \"TF-IDF\" ) . match ( from_list , to_list ) The resulting matches can be accessed through model.get_matches() : >>> model . get_matches () From To Similarity 0 apple apple 1.000000 1 apples apples 1.000000 2 appl apple 0.783751 3 recal None 0.000000 4 house mouse 0.587927 5 similarity None 0.000000 NOTE : When instantiating PolyFuzz we also could have used \"EditDistance\" or \"Embeddings\" to quickly access Levenshtein and FastText (English) respectively.","title":"Getting Started"},{"location":"tutorial/quickstart/quickstart/#fit-transform","text":"The .match function allows you to quickly extract similar strings. However, after selecting the right models to be used, you may want to use PolyFuzz in production to match incoming strings. To do so, we can make use of the familiar fit , transform , and fit_transform functions. Let's say that we have a list of words that we know to be correct called train_words . We want to any incoming word to mapped to one of the words in train_words . In other words, we fit on train_words and we use transform on any incoming words: from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from polyfuzz import PolyFuzz train_words = [ \"apple\" , \"apples\" , \"appl\" , \"recal\" , \"house\" , \"similarity\" ] unseen_words = [ \"apple\" , \"apples\" , \"mouse\" ] # Fit model = PolyFuzz ( \"TF-IDF\" ) model . fit ( train_words ) # Transform results = model . transform ( unseen_words ) In the above example, we are using fit on train_words to calculate the TF-IDF representation of those words which are saved to be used again in transform . This speeds up transform quite a bit since all TF-IDF representations are stored when applying fit .","title":"Fit / Transform"},{"location":"tutorial/quickstart/quickstart/#save-load","text":"We can save and load the model as follows to be used in production: # Save the model model . save ( \"my_model\" ) # Load the model loaded_model = PolyFuzz . load ( \"my_model\" )","title":"Save / Load"},{"location":"tutorial/quickstart/quickstart/#group-matches","text":"We can group the matches To as there might be significant overlap in strings in our to_list. To do this, we calculate the similarity within strings in to_list and use single linkage to then group the strings with a high similarity. When we extract the new matches, we can see an additional column Group in which all the To matches were grouped to: >>> model . group ( link_min_similarity = 0.75 ) >>> model . get_matches () From To Similarity Group 0 apple apple 1.000000 apples 1 apples apples 1.000000 apples 2 appl apple 0.783751 apples 3 recal None 0.000000 None 4 house mouse 0.587927 mouse 5 similarity None 0.000000 None As can be seen above, we grouped apple and apples together to apple such that when a string is mapped to apple it will fall in the cluster of [apples, apple] and will be mapped to the first instance in the cluster which is apples .","title":"Group Matches"},{"location":"tutorial/quickstart/quickstart/#precision-recall-curve","text":"Next, we would like to see how well our model is doing on our data. We express our results as precision and recall where precision is defined as the minimum similarity score before a match is correct and recall the percentage of matches found at a certain minimum similarity score. Creating the visualizations is as simple as: model.visualize_precision_recall()","title":"Precision-Recall Curve"}]}